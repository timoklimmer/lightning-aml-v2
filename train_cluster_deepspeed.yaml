$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json

# name of the experiment, use this to structure your training results
experiment_name: pl-deepspeed

# several input parameters for your training job
inputs:
    # name of the model, ensure that there is a corresponding Python script in folder 'models'
    model: ImageClassifier

    # name of the data module
    # here I am using a custom data module which is a Fashion MNIST dataset
    # (not split into train and test, single csv file)
    # you can also use the built-in MNIST data module
    data_module: MNISTFromBlob

    # training strategy used by Lightning
    # - see https://pytorch-lightning.readthedocs.io/en/stable/extensions/strategy.html#id1 to get an idea of the
    #   available strategies
    # - for a full list of available strategies, run python train.py --help
    # - to use a custom DeepSpeed configuration, overwrite it in the training script like so:
    #       deepspeed_config = { "steps_per_print": 50, ... }
    #       strategy = pl.strategies.DeepSpeedStrategy(config=deepspeed_config)
    #       trainer = pl.Trainer(strategy=strategy,...)
    # - in this example, you can simply place a ds_config.json file to configure DeepSpeed. if no ds_config.json is
    #   provided, default values will be used.
    # - note that there are also built-in variations of the DeepSpeed strategy, eg. deepspeed_stage_3
    training_strategy: deepspeed

    # maximum number of epochs
    max_epochs: 400

    # Set the URI path for the data. Supported paths include:
    # local: `./<path>
    #  Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>
    # ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>
    # Datastore: azureml://datastores/<data_store_name>/paths/<path>
    # Data Asset: azureml:<my_data>:<version>
    # important: the compute cluster needs to have access to the data source
    # Storage Blob Data Reader role is the minimum required for the compute cluster for blob storage/ADLS
    # if you are using MNIST inbuilt module, please comment out the data_folder and batch_size

    data_folder:
        mode: ro_mount
        path: azureml://datastores/genstorage348/paths/files
        type: uri_folder

    #set the batch size for ml processing
    batch_size: 64
    #set to true to enable Nebula checkpointing
    enable_nebula: False


# compute cluster on which the command above should be run
# note: before you can use the compute, it needs to be created first in your AzureML workspace
compute: multi-node-multi-gpu-cc

# compute resources
resources:
    # number of nodes in the cluster above
    instance_count: 3

# communication between nodes
distribution:
    # when distribution type "pytorch" is used, AML will set up everything required for distributed training of PyTorch
    # models, for example the NCCL backend
    type: pytorch

    # IMPORTANT: set this value to the number of GPUs per device (depends on your VM size)
    process_count_per_instance: 4

# environment in which the command above should be run in
# note: It is recommended to use an environment which either is the ACPT image or at least inherits from it. The ACPT
#       image has many frameworks related to PyTorch pre-installed, hence reduces efforts to create your own
#       environment. To learn more about environments in AML, see
#       https://learn.microsoft.com/en-us/azure/machine-learning/concept-environments
#
# for the latest ACPT image using PyTorch 1.12, CUDA 11.6 on Python 3.9
environment: azureml:AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu@latest
# for a specific version
#environment: azureml:AzureML-ACPT-pytorch-1.12-py39-cuda11.6-gpu:3
# for the latest version of a custom environment:
#environment: azureml:ACPT-Extended@latest
# for a specific Docker image (has to be compatible with Azure ML):
#environment:
#    image: mcr.microsoft.com/azureml/curated/acpt-pytorch-1.12-py39-cuda11.6-gpu:2

# You can also define environment variables which will be available in your Python script (e.g., train.py)
# via the os.environ dictionary;
# Please refer here for best practices -->
# https://github.com/Azure/azureml-examples/blob/main/best-practices/largescale-deep-learning/Data-loading/data-loading.md;

# Here's an example of how you can reference a secret from Azure Key Vault in your YAML configuration:
# environment_variables:
#    MY_SECRET_API_KEY: ${{secrets.MY_SECRET_NAME}}

# command that should be run by the job
command: |
    python train.py \
        --model ${{inputs.model}} \
        --data-module ${{inputs.data_module}} \
        --training-strategy ${{inputs.training_strategy}} \
        --max-epochs ${{inputs.max_epochs}} \
        --data_folder ${{inputs.data_folder}} \
        --batch_size ${{inputs.batch_size}} \
        --enable_nebula ${{inputs.enable_nebula}}

# local directory that contains the code for the job
# note: By default, all contents of the specified directory are uploaded to Azure ML. If you want to avoid that specific
#       files or folders are uploaded, add an .amlignore file to that directory. Contents and syntax is the same as with
#       .gitignore files.
code: .
# TODO: update
# # optional: side services
# # - the services defined here are services running in parallel to the job
# # - for more details, see https://learn.microsoft.com/en-us/azure/machine-learning/how-to-interactive-jobs
# # - feature is still in preview, hence might not work as expected in certain situations
# services:
#     my_vs_code:
#         job_service_type: vs_code
#         nodes: all
#     my_tensor_board:
#         job_service_type: tensor_board
#         properties:
#             logDir: "outputs"
#         nodes: all
#     my_jupyter_lab:
#         job_service_type: jupyter_lab
#         nodes: all
#     my_ssh:
#         job_service_type: ssh
#         properties:
#             sshPublicKeys: <paste the entire pub key content>
#         nodes: all
